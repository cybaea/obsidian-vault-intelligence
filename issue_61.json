{"body":"For our RAG, we should implement chunking of the markdown documents.\n\nChunking: the Markdown content is split into contextual “chunks” (for example, each section or subsection of a document might become a chunk).\n\nEach chunk then can have an embedding score. When we get to identify the context for the RAG, we can use the chunks.\n\nTEST if this is better. Hypothesis: it will be better if you have many large documents as context. If you have smaller documents, say two chunks each, it is _probably_ better to pass the whole document as context.\n\nHOW do we implement an algorithm to optimise this?? That feels like a **hard** problem...\n\nSee also:\n\n- https://ragnar.tidyverse.org/reference/markdown_chunk.html\n","comments":[{"id":"IC_kwDOQwjrps7lPaKd","author":{"login":"cybaea"},"authorAssociation":"OWNER","body":"## Technical Implementation Proposal (from redundant Issue #167)\n\n### Context\nDiscovered during an investigation where a 'Cat story' deep in a file was not found by the Research Agent. We mitigated the *Keyword* search by increasing the string index limit, but the *Semantic* search limitation remains until chunking is implemented.\n\n### Current Limitation\nThe current vector search implementation generates a single embedding for each file. Most local embedding models have a strict token limit (e.g., 512 tokens, approx 2000 chars).\nFiles larger than this limit are effectively truncated for semantic search purposes. Content located deep in the file will completely disappear from the vector space.\n\n### Proposed Solution\nRefactor `indexer.worker.ts` and `GraphService.ts` to implement a **Chunking Strategy**.\n\n1.  **Ingestion**:\n    *   Split large content into overlapping text chunks (e.g., 512 tokens with 50-token overlap).\n    *   Generate independent embeddings for each chunk.\n    *   Store chunks in Orama. The schema likely needs to support a `parentId` or `chunkId` to map back to the source `path`.\n\n2.  **Search**:\n    *   Query the vector index to find matching *chunks*.\n    *   Map matches back to the unique source files.\n    *   (Optional) Use the specific matched chunk for more precise context assembly in `ContextAssembler`.","createdAt":"2026-02-04T08:21:47Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/cybaea/obsidian-vault-intelligence/issues/61#issuecomment-3846021789","viewerDidAuthor":true},{"id":"IC_kwDOQwjrps7lPh1i","author":{"login":"cybaea"},"authorAssociation":"OWNER","body":"## Optimized Implementation Plan for Gemini (February 2026)\n\nBased on the [Gemini API Embedding Documentation](https://ai.google.dev/gemini-api/docs/embeddings) and our default model (`gemini-embedding-001`), the implementation must adhere to the following specific constraints and optimization strategies:\n\n### 1. Hard Limits\n*   **Max Input Tokens**: 2,048 tokens per input.\n*   **Batch Size**: Up to 250 text chunks per API call (`embedContent` or `batchEmbedContents`).\n\n### 2. Chunking Strategy\nAlthough the hard limit is 2,048 tokens, we will **NOT** use the full limit for chunking. Large chunks dilute semantic density, making retrieval less precise.\n\n*   **Target Chunk Size**: **256 - 512 tokens** (approx. 1,000 - 2,000 characters).\n*   **Overlap**: **10% - 15%** (approx. 50 tokens).\n    *   *Rationale*: Prevents cutting off sentences/context at boundaries and ensures continuity.\n\n### 3. Implementation Details\n*   **Configuration**: Update `WORKER_CONSTANTS.MAX_TOKENS` or create a new constant `GEMINI_EMBEDDING_CHUNK_SIZE` set to `512`.\n*   **Batching**: Implement batching logic in `GeminiService.embedText` (or a new `embedBatch`) to utilize the 250-chunk concurrency limit for faster initial indexing.\n*   **Orama Storage**: Store a parent-child relationship:\n    *   `ParentNode`: The file metadata.\n    *   `ChildNode`: The chunk text + embedding + reference to Parent.\n","createdAt":"2026-02-04T08:29:14Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/cybaea/obsidian-vault-intelligence/issues/61#issuecomment-3846053218","viewerDidAuthor":true},{"id":"IC_kwDOQwjrps7lPmKw","author":{"login":"cybaea"},"authorAssociation":"OWNER","body":"## updated Splitting Strategy (No new external dependencies)\n\nWe should **avoid** adding `@langchain/textsplitters` to keep the plugin lightweight. We already have robust parsing utilities in `src/utils/link-parsing.ts`.\n\n### 1. Handling YAML Headers (Frontmatter)\n*   **Existing Utility**: Use `splitFrontmatter(text)` from `src/utils/link-parsing.ts`.\n*   **Strategy**:\n    1.  Extract `frontmatter` once.\n    2.  Prepend the `frontmatter` (or a summarized version of it) to **every** chunk.\n    *   *Why?* The frontmatter often contains critical context (tags, aliases, creation dates) that applies to the *entire* document. A chunk from the middle of the file might lose meaning without this header context.\n\n### 2. Body Splitting (Recursive Character Splitter)\nSince we are not importing LangChain, we will implement a lightweight `recursiveCharacterSplitter` function.\n*   **Separators**: `[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]`.\n*   **Logic**:\n    *   Try to split by paragraphs (`\\n\\n`) first.\n    *   If a paragraph exceeds the 512-token limit, split by lines (`\\n`).\n    *   If a line exceeds the limit, split by sentences, and so on.\n*   **Length Function**: Use a simple character approximation (4 chars ~= 1 token) or the existing tokenizer if available in the worker.","createdAt":"2026-02-04T08:33:40Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/cybaea/obsidian-vault-intelligence/issues/61#issuecomment-3846070960","viewerDidAuthor":true},{"id":"IC_kwDOQwjrps7lPrau","author":{"login":"cybaea"},"authorAssociation":"OWNER","body":"## Refined Context Strategy (User Feedback Implemented)\n\nWe will **NOT** prepend raw YAML frontmatter to chunks, as it dilutes semantic density and risks exceeding token limits. Instead, we will adopt a **Hybrid Metadata + Natural Language Context** approach.\n\n### 1. Hybrid Search (Metadata Filtering)\n*   **Mechanism**: Store structured metadata fields directly in the Orama schema, separate from the embedding.\n*   **Fields**: `created`, `modified`, `author`, `status`, `folder`.\n*   **Benefit**: Enables precise filtering (e.g., \"Find notes about 'AI' created after 2024\") without confusing the vector model with raw dates.\n\n### 2. Natural Language Context Injection\n*   **Mechanism**: Convert high-value semantic frontmatter fields (`topics`, `title`, `tags`) into a short natural language sentence.\n*   **Format**: \"Document '{title}' covers topics: {topics}.\"\n*   **Implementation**: Prepend this sentence to **every** chunk before embedding.\n*   **Benefit**: Provides strong semantic grounding for isolated chunks without the noise of YAML syntax.\n\n### 3. Revised Orama Schema\n```typescript\nschema: {\n    content: 'string',\n    embedding: 'vector[...] ',\n    path: 'enum',\n    title: 'string',\n    // New Metadata Fields\n    created: 'number',\n    params: 'string[]', // For tags/topics\n    status: 'string'\n}\n```","createdAt":"2026-02-04T08:39:07Z","includesCreatedEdit":false,"isMinimized":false,"minimizedReason":"","reactionGroups":[],"url":"https://github.com/cybaea/obsidian-vault-intelligence/issues/61#issuecomment-3846092462","viewerDidAuthor":true}],"title":"Implement better chuncking of markdown documents for RAG"}
